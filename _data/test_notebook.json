{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "\n",
    "Make a new Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "Save it or copy it to the _data folder in the project pages site directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "Rename it from *.ipynb to .json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Create a new post (.md file in _posts directory). It's YAML front matter should look something like this:\n",
    "```\n",
    "---\n",
    "layout:     notebook\n",
    "title:      How to Make a Notebook Post\n",
    "author:     Dano Morrison\n",
    "tags: \t\tjupyter\n",
    "subtitle:   It's as easy as dragging a notebook into the _data folder\n",
    "category:  examples\n",
    "\n",
    "notebookfilename: test_notebook\n",
    "visualworkflow: true\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5:\n",
    "Build the site (push to GitHub or `jekyll serve`). Everything else is taken care of for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some serious notebook stuff to check out to test the formatting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTRegressor\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has been collected, let's import it\n",
    "\n",
    "data = pd.read_csv(\"../muse-data/DanoDenoisedPSDJul18.csv\", header=0, index_col=False)\n",
    "\n",
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH_LENGTH = 440 # 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get our labels data set first because it's easier. We'll grab every 4th row from the Performance column\n",
    "\n",
    "labels = data['Performance'].iloc[::4]\n",
    "\n",
    "# Then we'll reindex the dataframe\n",
    "\n",
    "labels = labels.reset_index().drop('index', axis=1)\n",
    "\n",
    "# Convert to 1D array for TPOT\n",
    "\n",
    "labels = np.array(labels).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperate data into 4 dataframes, 1 for each electrode\n",
    "\n",
    "chan1 = data.loc[:,'Channel':'60 hz'].loc[data['Channel'] == 1,].reset_index(drop=True)\n",
    "chan1.columns = np.arange(1000,1061)\n",
    "chan2 = data.loc[:,'Channel':'60 hz'].loc[data['Channel'] == 2,].reset_index(drop=True)\n",
    "chan2.columns = np.arange(2000,2061)\n",
    "chan3 = data.loc[:,'Channel':'60 hz'].loc[data['Channel'] == 3,].reset_index(drop=True)\n",
    "chan3.columns = np.arange(3000,3061)\n",
    "chan4 = data.loc[:,'Channel':'60 hz'].loc[data['Channel'] == 4,].reset_index(drop=True)\n",
    "chan4.columns = np.arange(4000,4061)\n",
    "\n",
    "\n",
    "# Concat all channel-specific dataframes together so that row = 2s epoch\n",
    "# columns = [electrode 1 FFT bins] + [electrode 2 FFT bins] + ...\n",
    "training_data = pd.concat([chan1.iloc[:,1:], chan2.iloc[:,1:], chan3.iloc[:,1:], chan4.iloc[:,1:]], axis=1, join_axes=[chan1.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(training_data.shape)\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TPOTClassifier that will run for 10 generations\n",
    "\n",
    "pipeline_optimizer = TPOTRegressor(generations=10, population_size=30, cv=5,\n",
    "                                    random_state=42, verbosity=3) \n",
    "\n",
    "# Fit this baby! Takes a long time to run\n",
    "\n",
    "pipeline_optimizer.fit(training_data, labels)  \n",
    "  \n",
    "# See what kind of score we get\n",
    "print(pipeline_optimizer.score(training_data, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
